import os
import yaml
import pandas as pd
from vanna.qianwen.QianwenAI_chat import QianWenAI_Chat
from vanna.chromadb import ChromaDB_VectorStore
import json
import time
from contextlib import contextmanager
from threading import Thread
from rapidfuzz import fuzz
import re
from pandasql import sqldf

# æ ¹æ®é…ç½®åŠ¨æ€å¯¼å…¥æ•°æ®åº“é©±åŠ¨
try:
    import mysql.connector
    from mysql.connector.pooling import MySQLConnectionPool
    MYSQL_AVAILABLE = True
except ImportError:
    MYSQL_AVAILABLE = False

try:
    import pyodbc
    PYODBC_AVAILABLE = True
except ImportError:
    PYODBC_AVAILABLE = False

# ç®€å•çš„SQL Serverè¿æ¥æ± å®ç°
class SimpleConnectionPool:
    def __init__(self, max_connections, connection_creator):
        self.max_connections = max_connections
        self.connection_creator = connection_creator
        self.connections = []
        self.in_use = set()
    
    def get_connection(self):
        # é¦–å…ˆæ¸…ç†æ— æ•ˆå¼•ç”¨
        self.in_use = {conn for conn in self.in_use if conn in self.connections}
        
        # å°è¯•ä»ç°æœ‰è¿æ¥ä¸­æ‰¾åˆ°æœªä½¿ç”¨çš„
        for i in range(len(self.connections) - 1, -1, -1):  # åå‘éå†ä»¥å®‰å…¨ç§»é™¤
            conn = self.connections[i]
            if conn not in self.in_use:
                try:
                    # pyodbcè¿æ¥éœ€è¦æ‰§è¡ŒæŸ¥è¯¢æ¥æ£€æŸ¥æ˜¯å¦æœ‰æ•ˆ
                    cursor = conn.cursor()
                    cursor.execute("SELECT 1")
                    cursor.close()
                    self.in_use.add(conn)
                    return conn
                except Exception as e:
                    print(f"è¿æ¥æ± ä¸­çš„è¿æ¥å·²å¤±æ•ˆï¼Œç§»é™¤: {e}")
                    # è¿æ¥å¤±æ•ˆï¼Œç§»é™¤
                    try:
                        self.connections.pop(i)
                    except:
                        pass
        
        # åˆ›å»ºæ–°è¿æ¥
        if len(self.connections) < self.max_connections:
            try:
                conn = self.connection_creator()
                self.connections.append(conn)
                self.in_use.add(conn)
                return conn
            except Exception as e:
                print(f"åˆ›å»ºæ–°è¿æ¥å¤±è´¥: {e}")
                raise
        
        # è¿æ¥æ± å·²æ»¡ï¼Œæ‰“å°å½“å‰çŠ¶æ€
        print(f"è¿æ¥æ± å·²æ»¡: æ€»è¿æ¥æ•°={len(self.connections)}, ä½¿ç”¨ä¸­={len(self.in_use)}")
        raise Exception("è¿æ¥æ± å·²æ»¡ï¼Œæ— æ³•è·å–æ–°è¿æ¥")
    
    def release(self, connection):
        if connection in self.in_use:
            self.in_use.remove(connection)
            print(f"è¿æ¥å·²é‡Šæ”¾å›è¿æ¥æ± ï¼Œå½“å‰ä½¿ç”¨ä¸­è¿æ¥: {len(self.in_use)}/{len(self.connections)}")
    
    def close_all(self):
        for conn in self.connections:
            try:
                conn.close()
            except Exception as e:
                print(f"å…³é—­è¿æ¥æ—¶å‡ºé”™: {e}")
        self.connections = []
        self.in_use = set()
        print("æ‰€æœ‰æ•°æ®åº“è¿æ¥å·²å…³é—­")

# Load configuration from YAML file
def load_config():
    with open('config.yaml', 'r', encoding='utf-8') as file:
        return yaml.safe_load(file)

class MyVanna(QianWenAI_Chat, ChromaDB_VectorStore):
    def _get_common_messages(self, **kwargs) -> list:
        """
        [æ–°å¢] å®ç°åŸºç±»ä¸­ç¼ºå¤±çš„é€šç”¨æ¶ˆæ¯æ–¹æ³•ï¼Œæä¾›ä¸€ä¸ªé»˜è®¤çš„ç³»ç»ŸæŒ‡ä»¤ã€‚
        è¿™ä¿®å¤äº†åœ¨ generate_sql å’Œ correct_sql ä¸­å‘ç”Ÿçš„ 'AttributeError'ã€‚
        """
        return [{"role": "system", "content": "You are a professional SQL assistant. Given the user's question and context, generate a single, executable SQL query for the specified database dialect. Do not add any explanations or markdown."}]

    def _format_rag_context_for_prompt(self, rag_context_obj: dict) -> str:
        """
        [æ–°å¢] å°†RAGä¸Šä¸‹æ–‡å¯¹è±¡æ ¼å¼åŒ–ä¸ºå•ä¸ªå­—ç¬¦ä¸²ï¼Œç”¨äºæ³¨å…¥åˆ°æç¤ºä¸­ã€‚
        æ­¤è¾…åŠ©æ–¹æ³•ç¡®ä¿äº†è°ƒè¯•è¾“å‡ºå’Œæç¤ºæ„å»ºçš„ä¸€è‡´æ€§ã€‚
        """
        context_parts = []
        if rag_context_obj.get("station_info_context"):
            context_parts.append(f"--- ç›¸å…³ç«™ç‚¹ä¿¡æ¯ (Station Info) ---\n{rag_context_obj['station_info_context']}")
        if rag_context_obj.get("ddl_context"):
            context_parts.append(f"--- ç›¸å…³è¡¨ç»“æ„ (DDL) ---\n{rag_context_obj['ddl_context']}")
        if rag_context_obj.get("doc_context"):
            context_parts.append(f"--- ç›¸å…³ä¸šåŠ¡çŸ¥è¯† (General Documentation) ---\n{rag_context_obj['doc_context']}")
        if rag_context_obj.get("sql_context"):
            context_parts.append(f"--- ç±»ä¼¼çš„æŸ¥è¯¢èŒƒä¾‹ (Similar SQL Queries) ---\n{rag_context_obj['sql_context']}")
        
        return "\n\n".join(context_parts) if context_parts else "æ— ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚"

    def _clean_llm_response(self, response: str) -> str:
        """
        A robust function to clean various forms of cruft from LLM-generated SQL.
        Handles markdown, language identifiers, and leading/trailing whitespace.
        """
        # 1. Strip leading/trailing whitespace
        cleaned_str = response.strip()

        # 2. Handle markdown blocks (e.g., ```sql ... ```)
        if cleaned_str.startswith("```") and cleaned_str.endswith("```"):
            # Find the first newline
            first_newline = cleaned_str.find('\n')
            if first_newline != -1:
                # Take everything after the first line (which contains ```sql)
                cleaned_str = cleaned_str[first_newline + 1:]
            # Remove the trailing ```
            cleaned_str = cleaned_str.rsplit('```', 1)[0].strip()
        
        # 3. Handle cases where the response starts with 'sql' or 'json' identifier
        if cleaned_str.lower().startswith('sql'):
            cleaned_str = cleaned_str[3:].lstrip() # Remove 'sql' and any following whitespace/newline

        return cleaned_str

    def stringify_history(self, history: list) -> str:
        """
        [æ–°å¢] å°†å¯¹è¯å†å²åˆ—è¡¨è½¬æ¢ä¸ºæ ¼å¼åŒ–çš„å­—ç¬¦ä¸²ã€‚
        """
        if not history:
            return "æ— å†å²å¯¹è¯"
            
        history_str_parts = []
        for turn in history:
            role = "ç”¨æˆ·" if turn.get("role") == "user" else "AIåŠ©æ‰‹"
            content = turn.get("content", "")
            
            # å°è¯•è§£æå†…å®¹ï¼Œçœ‹å®ƒæ˜¯å¦æ˜¯æ¨¡å‹è¿”å›çš„JSONå¯¹è±¡
            try:
                content_json = json.loads(content)
                if isinstance(content_json, dict):
                    # å¦‚æœæ˜¯æ¾„æ¸…é—®é¢˜ï¼Œå°±æå–é—®é¢˜æ–‡æœ¬
                    if 'clarification_needed' in content_json:
                        content = f"è¯·æ±‚æ¾„æ¸…: {content_json['clarification_needed']}"
                    # å¦‚æœæ˜¯SQLï¼Œå°±æ ¼å¼åŒ–å±•ç¤º
                    elif 'sql' in content_json:
                        content = f"ç”Ÿæˆäº†SQL:\n```sql\n{content_json['sql']}\n```"
            except (json.JSONDecodeError, TypeError):
                # å†…å®¹ä¸æ˜¯JSONï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ–‡æœ¬
                pass
                
            history_str_parts.append(f"{role}: {content}")
            
        return "\n".join(history_str_parts)

    def _extract_entities_for_rag(self, question: str) -> dict:
        """
        [é‡æ„] ä½¿ç”¨LLMä»é—®é¢˜ä¸­æå–å…³é”®å®ä½“ï¼Œå¹¶è¿”å›ä¸€ä¸ªç»“æ„åŒ–çš„å­—å…¸ã€‚
        ä¸Šä¸‹æ–‡ä¿¡æ¯ç°åœ¨ä» self.table_metadata å’Œ self.field_mappings åŠ¨æ€æ„å»ºã€‚
        è¾“å‡ºè¢«é‡æ„æˆä¸€ä¸ªJSONå¯¹è±¡ï¼Œä»¥åˆ†ç±»å®ä½“ã€‚
        """
        print("æ­£åœ¨æå–é—®é¢˜ä¸­çš„å…³é”®å®ä½“...")

        # --- 1. åŠ¨æ€æ„å»ºä¸Šä¸‹æ–‡ ---
        try:
            # æ„å»ºå…³é”®è¡¨ä¿¡æ¯å­—ç¬¦ä¸²
            table_info_parts = []
            for name, meta in self.table_metadata.items():
                part = f"- **{name} ({meta.get('business_name', 'N/A')})**: {meta.get('description', 'N/A')}"
                table_info_parts.append(part)
            key_table_info_str = "\n".join(table_info_parts)

            # æ„å»ºå…³é”®å­—æ®µä¿¡æ¯å­—ç¬¦ä¸² (é€‰æ‹©æ€§åœ°å±•ç¤ºéƒ¨åˆ†ï¼Œé¿å…è¿‡é•¿)
            field_info_parts = []
            # ç®€å•åœ°å–å‰ N ä¸ªæˆ–æ ¹æ®æŸç§é€»è¾‘é€‰æ‹©
            for field, desc in list(self.field_mappings.items())[:30]: # å–å‰30ä¸ªä½œä¸ºç¤ºä¾‹
                field_info_parts.append(f"- **{field}**: {desc}")
            key_field_info_str = "\n".join(field_info_parts)
        except Exception as e:
            print(f"[è­¦å‘Š] åŠ¨æ€æ„å»ºå®ä½“æå–ä¸Šä¸‹æ–‡æ—¶å‡ºé”™: {e}. å°†ä½¿ç”¨åŸºç¡€æç¤ºã€‚")
            key_table_info_str = "æ— "
            key_field_info_str = "æ— "

        # --- 2. æ„å»ºåŠ¨æ€ Prompt ---
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®åº“å…ƒæ•°æ®åˆ†æå™¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€é—®é¢˜ä¸­ï¼Œæå–å‡ºæ‰€æœ‰å¯èƒ½ä¸æ•°æ®åº“æŸ¥è¯¢ç›¸å…³çš„å…³é”®è¯ã€‚

è¯·å‚è€ƒä»¥ä¸‹åŠ¨æ€ç”Ÿæˆçš„æ•°æ®åº“å…³é”®è¡¨ä¿¡æ¯å’Œå­—æ®µé‡Šä¹‰ï¼Œä»¥æé«˜ä½ æå–çš„å‡†ç¡®æ€§ã€‚

### å…³é”®è¡¨ä¿¡æ¯:
{key_table_info_str}

### éƒ¨åˆ†å…³é”®å­—æ®µé‡Šä¹‰:
{key_field_info_str}

ç”¨æˆ·é—®é¢˜: "{question}"

è¯·åˆ†æä»¥ä¸Šé—®é¢˜ï¼Œå¹¶**åªè¿”å›ä¸€ä¸ªJSONå¯¹è±¡å­—ç¬¦ä¸²**ï¼Œç”¨äºåˆ†ç±»æå–å‡ºçš„å®ä½“ã€‚
JSONå¯¹è±¡åº”åŒ…å«ä»¥ä¸‹é”®ï¼š "locations", "tables", "columns", "time_expressions", "other_terms"ã€‚
- "locations": åœ°ç‚¹æˆ–ç«™ç‚¹åç§° (ä¾‹å¦‚ 'å‡¤å‡°å±±', 'å¹¿å·'ï¼Œ'å¤©æ²³åŒº')ã€‚
- "tables": æ•°æ®åº“è¡¨åæˆ–å…¶ä¸šåŠ¡åŒä¹‰è¯ (ä¾‹å¦‚ 'ç«™ç‚¹æ—¥å‡å€¼è¡¨', 'dat_station_day')ã€‚
- "columns": æ•°æ®åº“åˆ—åæˆ–å…¶ä¸šåŠ¡åŒä¹‰è¯ (ä¾‹å¦‚ 'AQI', 'é¦–è¦æ±¡æŸ“ç‰©')ã€‚
- "time_expressions": æ—¶é—´ç›¸å…³çš„è¯ (ä¾‹å¦‚ 'æ˜¨å¤©', '2025å¹´3æœˆ1æ—¥')ã€‚
- "other_terms": å…¶ä»–æ— æ³•å½’å…¥ä»¥ä¸Šåˆ†ç±»çš„ä¸šåŠ¡æœ¯è¯­ (ä¾‹å¦‚ 'ç©ºæ°”è´¨é‡', 'ä¸¥é‡æ±¡æŸ“')ã€‚

å¦‚æœæŸä¸ªç±»åˆ«æ²¡æœ‰è¯†åˆ«åˆ°å®ä½“ï¼Œè¯·è¿”å›ä¸€ä¸ªç©ºæ•°ç»„ `[]` ä½œä¸ºå…¶å€¼ã€‚
**è¿”å›æ ¼å¼ç¤ºä¾‹**: {{"locations": ["å‡¤å‡°å±±"], "tables": ["dat_station_day"], "columns": ["aqi"], "time_expressions": ["2025å¹´3æœˆ1æ—¥"], "other_terms": ["ç©ºæ°”è´¨é‡æ•°æ®"]}}
"""
        try:
            # æ³¨æ„ï¼šæ­¤å¤„ç›´æ¥è°ƒç”¨äº†çˆ¶ç±»çš„ submit_prompt
            response_str = self.submit_prompt([{"role": "user", "content": prompt}])
            
            # æ¸…ç†å¹¶è§£æJSON
            start = response_str.find('{')
            end = response_str.rfind('}')
            if start != -1 and end != -1:
                json_str = response_str[start:end+1]
                entities = json.loads(json_str)

                # ç¡®ä¿è¿”å›çš„æ˜¯å­—å…¸ï¼Œä¸”åŒ…å«é¢„æœŸçš„é”®
                if isinstance(entities, dict):
                    print(f"æˆåŠŸæå–å¹¶åˆ†ç±»å®ä½“: {entities}")
                    # éªŒè¯å¹¶è¡¥å……ç¼ºå¤±çš„é”®
                    for key in ["locations", "tables", "columns", "time_expressions", "other_terms"]:
                        if key not in entities:
                            entities[key] = []
                    return entities
                else:
                    print(f"è­¦å‘Š: LLMè¿”å›çš„ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡ï¼Œè€Œæ˜¯ä¸€ä¸ª: {type(entities)}")
                    return {}
            else:
                print("è­¦å‘Š: æœªèƒ½ä»LLMå“åº”ä¸­è§£æå‡ºå®ä½“JSONã€‚")
                return {}
        except Exception as e:
            print(f"é”™è¯¯: æå–å®ä½“æ—¶å‡ºé”™ - {e}")
            return {}

    def __init__(self, config=None):
        """
        Initializes the MyVanna service by correctly unpacking the nested configuration
        and initializing the parent classes in the correct order.
        """
        # --- Final and Correct Initialization Logic ---

        # 1. Start with a base config and define defaults
        service_config = {
            "model": "qwen-plus",
            "path": "./vanna_chroma_db",
        }

        # 2. Unpack the nested config from the YAML file into a flat dictionary
        if config:
            # Unpack the 'llm' section and update the service_config
            llm_config = config.get('llm', {})
            service_config.update(llm_config)

            # Unpack other sections if they exist
            if 'vector_store' in config:
                service_config.update(config.get('vector_store'))
            
            # This ensures the final config passed to initializers is flat
            # and contains the api_key at the top level where the initializer expects it.

        # 3. Initialize the parent classes in the correct order,
        #    passing the prepared, flat configuration.
        QianWenAI_Chat.__init__(self, config=service_config)
        ChromaDB_VectorStore.__init__(self, config=service_config)

        # 4. Initialize local database connection attributes
        if config:
            # [ä¿®æ”¹] å¢å¼ºæ•°æ®åº“é…ç½®çš„è¯»å–é€»è¾‘
            db_config = config.get('database', {})
            # ä¼˜å…ˆä½¿ç”¨æ–°çš„åµŒå¥—ç»“æ„ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œåˆ™å‡å®šä¸ºæ—§çš„æ‰å¹³ç»“æ„
            self.db_primary_connection_config = db_config.get('primary_connection')
            if self.db_primary_connection_config is None and db_config:
                print("æœªæ‰¾åˆ° 'primary_connection'ï¼Œå°†ä½¿ç”¨æ ¹ 'database' å¯¹è±¡ä½œä¸ºä¸»è¿æ¥é…ç½®ã€‚")
                self.db_primary_connection_config = db_config
            
            self.db_training_connections = db_config.get('training_connections', [])
            self.execution_mode = config.get('execution_mode', 'direct')
            
            # æ£€æµ‹ä¸»è¿æ¥çš„æ•°æ®åº“ç±»å‹
            self.db_type = self._detect_db_type(self.db_primary_connection_config)
        else:
            self.db_primary_connection_config = None
            self.db_training_connections = []
            self.execution_mode = 'direct'
            self.db_type = None
        
        # 5. è¿æ¥æ± å’Œè¿æ¥ç®¡ç†ç›¸å…³å±æ€§ (åŸºäºä¸»è¿æ¥)
        self.use_connection_pool = config.get('use_connection_pool', True) if config else True
        self.max_connections = config.get('max_connections', 10) if config else 10  # å¢åŠ é»˜è®¤è¿æ¥æ•°
        self.connection_pool = None
        self.connection_retry_attempts = config.get('connection_retry_attempts', 3) if config else 3

        # 6. [æ–°å¢] è°ƒè¯•æ¨¡å¼
        self.debug_mode = config.get('debug', False) if config else False
        if self.debug_mode:
            print("ğŸš€ DEBUG MODE IS ENABLED. RAG context will be printed to the console.")
    
        # 7. [æ ¸å¿ƒä¿®æ”¹] é›†ä¸­ç®¡ç†ä¸šåŠ¡å…ƒæ•°æ®ï¼Œä½¿å…¶åœ¨æ•´ä¸ªå®ä¾‹ä¸­å¯ç”¨
        self.table_metadata = {
            "bsd_station": {"business_name": "ç«™ç‚¹ä¿¡æ¯è¡¨", "description": "å­˜å‚¨å„ä¸ªç›‘æµ‹ç«™ç‚¹çš„åŸºç¡€ä¿¡æ¯ï¼Œå¦‚åç§°ã€ç¼–ç ã€ç»çº¬åº¦ã€åœ°å€ã€çŠ¶æ€ç­‰ã€‚", "relations": "é€šè¿‡ `areacode` ä¸ `bsd_region` è¡¨å…³è”ã€‚"},
            "bsd_region": {"business_name": "åŒºåŸŸä¿¡æ¯è¡¨", "description": "å­˜å‚¨è¡Œæ”¿åŒºåŸŸçš„å±‚çº§å…³ç³»ï¼Œå¦‚çœã€å¸‚ã€åŒºã€‚", "relations": "å¯é€šè¿‡ `parentid` è¿›è¡Œè‡ªå…³è”ï¼Œå®ç°å±‚çº§æŸ¥è¯¢ã€‚"},
            "dat_station_day": {"business_name": "ç«™ç‚¹æ—¥å‡å€¼è¡¨", "description": "å­˜å‚¨æ¯ä¸ªç«™ç‚¹æ¯å¤©çš„å„ç±»æ±¡æŸ“ç‰©æµ“åº¦æ—¥å‡å€¼å’ŒAQIç›¸å…³æ•°æ®ã€‚", "relations": "é€šè¿‡ `code` ä¸ `bsd_station.stationcode` æˆ– `bsd_station.uniquecode` å…³è”ã€‚"},
            "dat_city_day": {"business_name": "åŸå¸‚æ—¥å‡å€¼è¡¨", "description": "å­˜å‚¨æ¯ä¸ªåŸå¸‚æ¯å¤©çš„æ±¡æŸ“ç‰©å¹³å‡æµ“åº¦å’ŒAQIã€‚", "relations": "é€šè¿‡ `code` ä¸ `bsd_region.areacode` å…³è”ã€‚"},
            "dat_station_hour": {"business_name": "ç«™ç‚¹å°æ—¶è¡¨", "description": "å­˜å‚¨æ¯ä¸ªç«™ç‚¹æ¯å°æ—¶çš„å„ç±»æ±¡æŸ“ç‰©æµ“åº¦ã€æ°”è±¡æ•°æ®ä»¥åŠAQIã€‚", "relations": "é€šè¿‡ `code` ä¸ `bsd_station.stationcode` æˆ– `bsd_station.uniquecode` å…³è”ã€‚è¡¨åé€šå¸¸æŒ‰å¹´ä»½åˆ‡åˆ†ï¼Œä¾‹å¦‚ `dat_station_hour_2024`ã€‚"},
            "dat_city_hour": {"business_name": "åŸå¸‚å°æ—¶è¡¨", "description": "å­˜å‚¨æ¯ä¸ªåŸå¸‚æ¯å°æ—¶çš„æ±¡æŸ“ç‰©æµ“åº¦ã€‚", "relations": "é€šè¿‡ `code` ä¸ `bsd_region.areacode` å…³è”ã€‚"}
        }
        self.field_mappings = {
            # --- é€šç”¨å­—æ®µ ---
            'id': 'è‡ªå¢ä¸»é”®',
            'name': 'åç§°ï¼ˆå¦‚ç«™ç‚¹/åŒºå¿/åŸå¸‚åç§°ï¼‰',
            'code': 'ç¼–ç ï¼ˆå¦‚ç«™ç‚¹/åŒºå¿/åŸå¸‚ç¼–ç ï¼‰',
            'areacode': 'åŒºåŸŸç¼–ç ',
            'areaname': 'åŒºåŸŸåç§°',
            'longitude': 'ç»åº¦',
            'latitude': 'çº¬åº¦',
            'timepoint': 'æ—¶é—´ç‚¹',
            'datatype': 'æ•°æ®ç±»å‹ï¼ˆ0=åŸå§‹å®å†µ, 1=å®¡æ ¸å®å†µ, 2=åŸå§‹æ ‡å†µ, 3=å®¡æ ¸æ ‡å†µï¼‰',
            'orderid': 'æ’åºID',
            'createtime': 'åˆ›å»ºæ—¶é—´',
            'createuser': 'åˆ›å»ºç”¨æˆ·',
            'updatetime': 'æ›´æ–°æ—¶é—´',
            'updateuser': 'æ›´æ–°ç”¨æˆ·',
            # --- bsd_station (ç«™ç‚¹ä¿¡æ¯è¡¨) ---
            'positionname': 'ç«™ç‚¹åç§°',
            'uniquecode': 'å”¯ä¸€ç¼–ç ',
            'stationcode': 'ç«™ç‚¹ç¼–ç ',
            'stationpic': 'ç«™ç‚¹å›¾ç‰‡è·¯å¾„',
            'address': 'ç«™ç‚¹åœ°å€',
            'pollutantcodes': 'æ±¡æŸ“ç‰©ç¼–ç åˆ—è¡¨',
            'stationtypeid': 'ç«™ç‚¹ç±»å‹ID',
            'status': 'çŠ¶æ€ï¼ˆå¦‚ç«™ç‚¹å¯ç”¨/ç¦ç”¨ï¼‰',
            'builddate': 'å»ºç«™æ—¥æœŸ',
            'phone': 'è”ç³»ç”µè¯',
            'manager': 'ç®¡ç†äººå‘˜',
            'description': 'æè¿°',
            'iscontrast': 'æ˜¯å¦ä¸ºå¯¹æ¯”ç«™ç‚¹',
            'ismonitor': 'æ˜¯å¦æ­£åœ¨ç›‘æµ‹',
            'ispublish': 'æ˜¯å¦å‘å¸ƒæ•°æ®',
            'stoptime': 'åœæ­¢ç›‘æµ‹æ—¶é—´',
            'mn': 'ç›‘æµ‹ç‚¹ç¼–å·',
            'parkcode': 'å›­åŒºç¼–ç ',
            'towncode': 'ä¹¡é•‡ç¼–ç ',
            'stationmultitypeid': 'ç«™ç‚¹å¤šç±»å‹ID',
            # --- bsd_region (åŒºåŸŸä¿¡æ¯è¡¨) ---
            'parentid': 'çˆ¶çº§åŒºåŸŸID',
            'areajc': 'åŒºåŸŸç®€ç§°',
            'level': 'åŒºåŸŸçº§åˆ«',
            'weathercode': 'æ°”è±¡ç¼–ç ',
            # --- æ•°æ®è¡¨ (dat_*) ---
            'so2': 'äºŒæ°§åŒ–ç¡«æµ“åº¦',
            'no2': 'äºŒæ°§åŒ–æ°®æµ“åº¦',
            'pm10': 'PM10æµ“åº¦',
            'co': 'ä¸€æ°§åŒ–ç¢³æµ“åº¦',
            'o3': 'è‡­æ°§æµ“åº¦',
            'pm2_5': 'PM2.5æµ“åº¦',
            'no': 'ä¸€æ°§åŒ–æ°®æµ“åº¦',
            'nox': 'æ°®æ°§åŒ–ç‰©æµ“åº¦',
            'o3_8h': 'è‡­æ°§8å°æ—¶æ»‘åŠ¨å¹³å‡å€¼',
            'windspeed': 'é£é€Ÿ',
            'winddirect': 'é£å‘',
            'pressure': 'æ°”å‹',
            'temperature': 'æ¸©åº¦',
            'humidity': 'æ¹¿åº¦',
            'rainfall': 'é™é›¨é‡',
            'visibility': 'èƒ½è§åº¦',
            'precipitation': 'é™æ°´é‡',
            'pm1': 'PM1æµ“åº¦',
            'co2': 'äºŒæ°§åŒ–ç¢³æµ“åº¦',
            'ch4': 'ç”²çƒ·æµ“åº¦',
            'thc': 'æ€»çƒƒæµ“åº¦',
            'nmhc': 'éç”²çƒ·çƒƒæµ“åº¦',
            'so2_mark': 'äºŒæ°§åŒ–ç¡«æ•°æ®æ ‡è®°',
            'no2_mark': 'äºŒæ°§åŒ–æ°®æ•°æ®æ ‡è®°',
            'pm10_mark': 'PM10æ•°æ®æ ‡è®°',
            'co_mark': 'ä¸€æ°§åŒ–ç¢³æ•°æ®æ ‡è®°',
            'o3_mark': 'è‡­æ°§æ•°æ®æ ‡è®°',
            'o3_8h_mark': 'è‡­æ°§8å°æ—¶æ•°æ®æ ‡è®°',
            'pm2_5_mark': 'PM2.5æ•°æ®æ ‡è®°',
            'no_mark': 'ä¸€æ°§åŒ–æ°®æ•°æ®æ ‡è®°',
            'nox_mark': 'æ°®æ°§åŒ–ç‰©æ•°æ®æ ‡è®°',
            'windspeed_mark': 'é£é€Ÿæ•°æ®æ ‡è®°',
            'winddirect_mark': 'é£å‘æ•°æ®æ ‡è®°',
            'pressure_mark': 'æ°”å‹æ•°æ®æ ‡è®°',
            'temperature_mark': 'æ¸©åº¦æ•°æ®æ ‡è®°',
            'humidity_mark': 'æ¹¿åº¦æ•°æ®æ ‡è®°',
            'rainfall_mark': 'é™é›¨é‡æ•°æ®æ ‡è®°',
            'visibility_mark': 'èƒ½è§åº¦æ•°æ®æ ‡è®°',
            'precipitation_mark': 'é™æ°´é‡æ•°æ®æ ‡è®°',
            'pm1_mark': 'PM1æ•°æ®æ ‡è®°',
            'co2_mark': 'äºŒæ°§åŒ–ç¢³æ•°æ®æ ‡è®°',
            'ch4_mark': 'ç”²çƒ·æ•°æ®æ ‡è®°',
            'thc_mark': 'æ€»çƒƒæ•°æ®æ ‡è®°',
            'nmhc_mark': 'éç”²çƒ·çƒƒæ•°æ®æ ‡è®°',
            'so2_iaqi': 'äºŒæ°§åŒ–ç¡«IAQIå€¼',
            'no2_iaqi': 'äºŒæ°§åŒ–æ°®IAQIå€¼',
            'pm10_iaqi': 'PM10 IAQIå€¼',
            'co_iaqi': 'ä¸€æ°§åŒ–ç¢³IAQIå€¼',
            'o3_iaqi': 'è‡­æ°§IAQIå€¼',
            'pm2_5_iaqi': 'PM2.5 IAQIå€¼',
            'aqi': 'ç©ºæ°”è´¨é‡æŒ‡æ•°ï¼ˆAQIï¼‰',
            'qualitytype': 'ç©ºæ°”è´¨é‡ç±»å‹ï¼ˆä¼˜ã€è‰¯ã€è½»åº¦æ±¡æŸ“ç­‰ï¼‰',
            'primarypollutant': 'é¦–è¦æ±¡æŸ“ç‰©'
        }

        # [æ–°å¢] ä»é…ç½®ä¸­åŠ è½½RAGç›¸å…³å‚æ•°ï¼Œä½œä¸º self.kwargs
        self.kwargs = config.get('rag_settings', {}) if config else {}

        # [æ–°å¢] æ¨¡ç³ŠåŒ¹é…é˜ˆå€¼
        self.similarity_threshold = config.get('fuzzy_matching', {}).get('similarity_threshold', 80)

        # [æ–°å¢] é€šç”¨å­—æ®µä¸šåŠ¡å«ä¹‰æ˜ å°„
        if config and 'field_mappings' in config:
            self.field_mappings.update(config['field_mappings'])

        # [æ–°å¢] å°†ç«™ç‚¹ä¿¡æ¯åŠ è½½åˆ°å†…å­˜
        self.station_info_data = []
        self._load_station_info()

    def _load_station_info(self, file_path='station_info.json'):
        """[æ–°å¢] å°†ç«™ç‚¹ä¿¡æ¯ä»JSONæ–‡ä»¶åŠ è½½åˆ°å†…å­˜ä¸­ã€‚"""
        print(f"--- æ­£åœ¨ä» '{file_path}' åŠ è½½ç«™ç‚¹ä¿¡æ¯åˆ°å†…å­˜ ---")
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data_obj = json.load(f)
            
            if isinstance(data_obj, dict) and 'results' in data_obj:
                self.station_info_data = data_obj['results']
                print(f"âœ… æˆåŠŸåŠ è½½ {len(self.station_info_data)} æ¡ç«™ç‚¹è®°å½•ã€‚")
            elif isinstance(data_obj, list):
                 self.station_info_data = data_obj
                 print(f"âœ… æˆåŠŸåŠ è½½ {len(self.station_info_data)} æ¡ç«™ç‚¹è®°å½•ã€‚")
            else:
                print(f"âš ï¸ è­¦å‘Š: æ— æ³•åœ¨ '{file_path}' ä¸­æ‰¾åˆ°ç«™ç‚¹åˆ—è¡¨ã€‚æœŸæœ›çš„æ ¼å¼æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæˆ–æ˜¯ä¸€ä¸ªåŒ…å« 'results' é”®çš„å­—å…¸ã€‚")
        except FileNotFoundError:
            print(f"âš ï¸ è­¦å‘Š: ç«™ç‚¹ä¿¡æ¯æ–‡ä»¶ '{file_path}' æœªæ‰¾åˆ°ã€‚ç¨‹åºå°†ç»§ç»­è¿è¡Œï¼Œä½†æ— æ³•æä¾›ç«™ç‚¹ä¸Šä¸‹æ–‡ã€‚")
        except json.JSONDecodeError as e:
            print(f"âŒ é”™è¯¯: è§£æç«™ç‚¹ä¿¡æ¯æ–‡ä»¶ '{file_path}' å¤±è´¥: {e}")
        except Exception as e:
            print(f"âŒ è¯»å–ç«™ç‚¹ä¿¡æ¯æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

    def _format_station_info_for_prompt(self, station: dict) -> str:
        """[æ–°å¢] å°†å•ä¸ªç«™ç‚¹å­—å…¸æ ¼å¼åŒ–ä¸ºç”¨äºæç¤ºè¯ä¸Šä¸‹æ–‡çš„å¯è¯»å­—ç¬¦ä¸²ã€‚"""
        name = station.get('ç«™ç‚¹åç§°')
        if not name: return ""
        
        parts = [f"å…³äºç«™ç‚¹'{name}'çš„ä¿¡æ¯ï¼š"]
        details = []
        uniquecode = station.get('å”¯ä¸€ç¼–ç ')
        longitude = station.get('ç»åº¦')
        latitude = station.get('çº¬åº¦')
        city = station.get('åŸå¸‚åç§°', 'æœªçŸ¥')
        station_type_id = station.get('ç«™ç‚¹ç±»å‹ID')

        if uniquecode: details.append(f"å…¶å”¯ä¸€ç¼–ç æ˜¯'{uniquecode}'")
        if longitude and latitude: details.append(f"åœ°ç†åæ ‡ä¸ºç»åº¦{longitude}ã€çº¬åº¦{latitude}")
        if city: details.append(f"æ‰€å±åŸå¸‚ä¸º'{city}'")
        if station_type_id is not None: details.append(f"ç«™ç‚¹ç±»å‹IDä¸º{station_type_id}")
        
        if not details: return ""
        return " ".join(parts) + "ï¼Œ".join(details) + "ã€‚"

    def _get_station_info_context_programmatically(self, entities: dict) -> str:
        """
        [é‡æ„] é€šè¿‡æ¨¡ç³ŠåŒ¹é…ä»å†…å­˜æ•°æ®ä¸­ç­›é€‰ç«™ç‚¹ä¿¡æ¯ã€‚
        ä½¿ç”¨ rapidfuzz è®¡ç®—ç›¸ä¼¼åº¦ï¼Œå¹¶æ ¹æ®é…ç½®çš„é˜ˆå€¼è¿›è¡Œç­›é€‰ã€‚
        """
        if not self.station_info_data:
            return "æ— ç«™ç‚¹ä¿¡æ¯å¯ä¾›æŸ¥è¯¢ã€‚"

        # ä»å®ä½“ä¸­æå–ç”¨äºæœç´¢çš„å…³é”®è¯
        raw_keywords = entities.get('locations', []) + entities.get('other_terms', [])
        if not raw_keywords:
            return "æœªåœ¨é—®é¢˜ä¸­è¯†åˆ«å‡ºéœ€è¦æŸ¥è¯¢çš„ç«™ç‚¹æˆ–åŸå¸‚ã€‚"
        
        # ä½¿ç”¨åŸå§‹å…³é”®è¯è¿›è¡Œæ—¥å¿—è®°å½•ï¼Œä¸è¿›è¡Œè§„èŒƒåŒ–
        print(f"æ­£åœ¨åŸºäºå…³é”®è¯ {raw_keywords} è¿›è¡Œæ¨¡ç³ŠåŒ¹é…...")
        
        matched_stations = []
        added_station_codes = set()

        for station in self.station_info_data:
            station_name = station.get('ç«™ç‚¹åç§°', '')
            city_name = station.get('åŸå¸‚åç§°', '')
            station_code = station.get('å”¯ä¸€ç¼–ç ')

            if not station_code or station_code in added_station_codes:
                continue
            
            # å¦‚æœæ²¡æœ‰åŸå¸‚å’Œç«™ç‚¹åï¼Œåˆ™è·³è¿‡
            if not station_name and not city_name:
                continue

            # [æ ¸å¿ƒä¿®æ”¹] è®¡ç®—æ¯ä¸ªå…³é”®è¯ä¸åŸå¸‚åå’Œç«™ç‚¹åçš„æœ€é«˜ç›¸ä¼¼åº¦
            # ä½¿ç”¨ ratio åŒ¹é…åŸå¸‚åï¼Œé€‚åˆåŒ¹é…å®Œæ•´çš„è¯
            best_city_score = max((fuzz.ratio(kw, city_name) for kw in raw_keywords if kw and city_name), default=0)
            
            # ä½¿ç”¨ partial_ratio åŒ¹é…ç«™ç‚¹åï¼Œé€‚åˆæŸ¥æ‰¾éƒ¨åˆ†åŒ¹é…
            best_station_score = max((fuzz.partial_ratio(kw, station_name) for kw in raw_keywords if kw and station_name), default=0)

            # å¦‚æœä»»ä¸€åˆ†æ•°è¾¾åˆ°é˜ˆå€¼ï¼Œåˆ™è®¤ä¸ºåŒ¹é…
            if best_city_score >= self.similarity_threshold or best_station_score >= self.similarity_threshold:
                matched_stations.append(station)
                added_station_codes.add(station_code)

        if not matched_stations:
            return f"æœªèƒ½æ ¹æ®å…³é”®è¯ '{', '.join(raw_keywords)}' å’Œç›¸ä¼¼åº¦é˜ˆå€¼ '{self.similarity_threshold}' æ‰¾åˆ°åŒ¹é…çš„ç«™ç‚¹ä¿¡æ¯ã€‚"
        
        print(f"âœ… æ¨¡ç³ŠåŒ¹é…æˆåŠŸï¼Œæ‰¾åˆ° {len(matched_stations)} ä¸ªç›¸å…³ç«™ç‚¹ã€‚")
        
        # æ ¼å¼åŒ–æ‰¾åˆ°çš„ç«™ç‚¹ä¿¡æ¯
        context_parts = [self._format_station_info_for_prompt(s) for s in matched_stations]
        # é™åˆ¶æœ€ç»ˆä¸Šä¸‹æ–‡çš„é•¿åº¦ï¼Œé˜²æ­¢è¶…å‡ºé™åˆ¶
        return "\n---\n".join(filter(None, context_parts[:20])) # æœ€å¤šè¿”å›20æ¡

    def _detect_db_type(self, db_config):
        """
        æ ¹æ®é…ç½®æ£€æµ‹æ•°æ®åº“ç±»å‹
        """
        if not db_config:
            return None
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ SQL Server ç‰¹æœ‰çš„é…ç½®é¡¹
        if 'driver' in db_config or 'server' in db_config or 'host' in db_config:
            if not PYODBC_AVAILABLE:
                raise ImportError("è¦è¿æ¥åˆ° SQL Serverï¼Œè¯·å®‰è£… pyodbc åŒ…: pip install pyodbc")
            return "sqlserver"
        
        # é»˜è®¤ä¸º MySQL
        if not MYSQL_AVAILABLE:
            raise ImportError("è¦è¿æ¥åˆ° MySQLï¼Œè¯·å®‰è£… mysql-connector-python åŒ…: pip install mysql-connector-python")
        return "mysql"

    def init_connection_pool(self):
        """åˆå§‹åŒ–è¿æ¥æ± """
        if not self.use_connection_pool or not self.db_primary_connection_config:
            print("æœªå¯ç”¨è¿æ¥æ± æˆ–ç¼ºå°‘ä¸»æ•°æ®åº“é…ç½®")
            return
            
        if self.connection_pool:
            # å·²å­˜åœ¨è¿æ¥æ± 
            print(f"è¿æ¥æ± å·²å­˜åœ¨ï¼Œè·³è¿‡åˆå§‹åŒ–ï¼Œå½“å‰è¿æ¥æ± : {type(self.connection_pool).__name__}")
            return
            
        try:
            if self.db_type == "mysql":
                if not MYSQL_AVAILABLE:
                    raise ImportError("è¦ä½¿ç”¨MySQLè¿æ¥æ± ï¼Œè¯·å®‰è£…mysql-connector-python")
                pool_config = self.db_primary_connection_config.copy()
                pool_config['pool_name'] = 'vanna_pool'
                pool_config['pool_size'] = self.max_connections
                self.connection_pool = MySQLConnectionPool(**pool_config)
                print(f"MySQLè¿æ¥æ± åˆå§‹åŒ–æˆåŠŸï¼Œæœ€å¤§è¿æ¥æ•°: {self.max_connections}")
            elif self.db_type == "sqlserver":
                if not PYODBC_AVAILABLE:
                    raise ImportError("è¦ä½¿ç”¨SQL Serverè¿æ¥æ± ï¼Œè¯·å®‰è£…pyodbc")
                    
                # åˆ›å»ºè¿æ¥å­—ç¬¦ä¸²å’Œè¿æ¥åˆ›å»ºå™¨
                def create_connection():
                    # [ä¿®æ”¹] ä½¿ç”¨æ­£ç¡®çš„é…ç½®é”®å¹¶å¤„ç†é—ç•™é”®
                    conn_str_parts = []
                    if 'driver' in self.db_primary_connection_config: conn_str_parts.append(f"DRIVER={self.db_primary_connection_config['driver']}")
                    if 'host' in self.db_primary_connection_config or 'server' in self.db_primary_connection_config: conn_str_parts.append(f"SERVER={self.db_primary_connection_config.get('host') or self.db_primary_connection_config.get('server')}")
                    if 'dbname' in self.db_primary_connection_config or 'database' in self.db_primary_connection_config: conn_str_parts.append(f"DATABASE={self.db_primary_connection_config.get('dbname') or self.db_primary_connection_config.get('database')}")
                    if 'user' in self.db_primary_connection_config or 'uid' in self.db_primary_connection_config: conn_str_parts.append(f"UID={self.db_primary_connection_config.get('user') or self.db_primary_connection_config.get('uid')}")
                    if 'password' in self.db_primary_connection_config or 'pwd' in self.db_primary_connection_config: conn_str_parts.append(f"PWD={self.db_primary_connection_config.get('password') or self.db_primary_connection_config.get('pwd')}")
                    
                    # [æœ€ç»ˆä¿®æ”¹] å¼ºåˆ¶å¯ç”¨åŠ å¯†å¹¶ä¿¡ä»»è¯ä¹¦ï¼Œä»¥æ¿€æ´» odbcinst.ini ä¸­çš„ MinTLS è®¾ç½®
                    conn_str_parts.append("TrustServerCertificate=yes")
                    conn_str_parts.append("Encrypt=yes")
                    conn_str = ";".join(conn_str_parts)
                    print(f"sql serveré“¾æ¥å­—ç¬¦ä¸²: {conn_str}") # [æ–°å¢] è°ƒè¯•è¾“å‡º

                    # å¢åŠ é‡è¯•é€»è¾‘
                    for attempt in range(1, self.connection_retry_attempts + 1):
                        try:
                            # å°è¯•è¿æ¥ï¼Œå¢åŠ è¶…æ—¶
                            print(f"æ­£åœ¨å°è¯•åˆ›å»ºæ–°è¿æ¥ (ç¬¬ {attempt}/{self.connection_retry_attempts} æ¬¡)...")
                            return pyodbc.connect(conn_str, timeout=5)
                        except pyodbc.Error as e:
                            print(f"åˆ›å»ºæ–°è¿æ¥å°è¯• {attempt}/{self.connection_retry_attempts} å¤±è´¥: {e}")
                            if attempt >= self.connection_retry_attempts:
                                raise  # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œé‡æ–°å¼•å‘å¼‚å¸¸
                            time.sleep(1) # ç­‰å¾…1ç§’å†é‡è¯•
                    
                self.connection_pool = SimpleConnectionPool(self.max_connections, create_connection)
                print(f"SQL Serverè¿æ¥æ± åˆå§‹åŒ–æˆåŠŸï¼Œæœ€å¤§è¿æ¥æ•°: {self.max_connections}")
        except Exception as e:
            print(f"è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥: {e}")
            self.connection_pool = None
            
    @contextmanager
    def get_db_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        connection = None
        retry_count = 0
        
        while retry_count < self.connection_retry_attempts:
            try:
                if self.use_connection_pool and self.connection_pool:
                    # ä½¿ç”¨è¿æ¥æ± è·å–è¿æ¥
                    if self.db_type == "mysql":
                        connection = self.connection_pool.get_connection()
                    else:  # sqlserver
                        connection = self.connection_pool.get_connection()
                else:
                    # ä¸ä½¿ç”¨è¿æ¥æ± ï¼Œç›´æ¥åˆ›å»ºè¿æ¥
                    if self.db_type == "mysql":
                        connection = mysql.connector.connect(**self.db_primary_connection_config)
                    elif self.db_type == "sqlserver":
                        # [ä¿®æ”¹] ä½¿ç”¨æ­£ç¡®çš„é…ç½®é”®å¹¶å¤„ç†é—ç•™é”®
                        conn_str_parts = []
                        if 'driver' in self.db_primary_connection_config: conn_str_parts.append(f"DRIVER={self.db_primary_connection_config['driver']}")
                        if 'host' in self.db_primary_connection_config or 'server' in self.db_primary_connection_config: conn_str_parts.append(f"SERVER={self.db_primary_connection_config.get('host') or self.db_primary_connection_config.get('server')}")
                        if 'dbname' in self.db_primary_connection_config or 'database' in self.db_primary_connection_config: conn_str_parts.append(f"DATABASE={self.db_primary_connection_config.get('dbname') or self.db_primary_connection_config.get('database')}")
                        if 'user' in self.db_primary_connection_config or 'uid' in self.db_primary_connection_config: conn_str_parts.append(f"UID={self.db_primary_connection_config.get('user') or self.db_primary_connection_config.get('uid')}")
                        if 'password' in self.db_primary_connection_config or 'pwd' in self.db_primary_connection_config: conn_str_parts.append(f"PWD={self.db_primary_connection_config.get('password') or self.db_primary_connection_config.get('pwd')}")
                        
                        # [æœ€ç»ˆä¿®æ”¹] å¼ºåˆ¶å¯ç”¨åŠ å¯†å¹¶ä¿¡ä»»è¯ä¹¦ï¼Œä»¥æ¿€æ´» odbcinst.ini ä¸­çš„ MinTLS è®¾ç½®
                        conn_str_parts.append("TrustServerCertificate=yes")
                        conn_str_parts.append("Encrypt=yes")
                        conn_str = ";".join(conn_str_parts)
                        print(f"sql serveré“¾æ¥å­—ç¬¦ä¸²: {conn_str}") # [æ–°å¢] è°ƒè¯•è¾“å‡º
                        connection = pyodbc.connect(conn_str, timeout=5)
                
                # éªŒè¯è¿æ¥æ˜¯å¦æœ‰æ•ˆ
                if self.db_type == "mysql" and not connection.is_connected():
                    raise Exception("MySQLè¿æ¥æ— æ•ˆ")
                elif self.db_type == "sqlserver":
                    # ä½¿ç”¨æŸ¥è¯¢æ¥éªŒè¯SQL Serverè¿æ¥
                    try:
                        cursor = connection.cursor()
                        cursor.execute("SELECT 1")
                        cursor.close()
                    except Exception as e:
                        raise Exception(f"SQL Serverè¿æ¥æ— æ•ˆ: {e}")
                    
                break  # è¿æ¥æˆåŠŸï¼Œé€€å‡ºé‡è¯•å¾ªç¯
            except Exception as e:
                retry_count += 1
                print(f"æ•°æ®åº“è¿æ¥å°è¯• {retry_count}/{self.connection_retry_attempts} å¤±è´¥: {e}")
                if retry_count >= self.connection_retry_attempts:
                    raise Exception(f"æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œå·²é‡è¯• {self.connection_retry_attempts} æ¬¡: {e}")
                time.sleep(1)  # é‡è¯•å‰ç­‰å¾…1ç§’
        
        try:
            yield connection
        finally:
            try:
                if self.use_connection_pool and self.connection_pool:
                    # å½’è¿˜è¿æ¥åˆ°è¿æ¥æ± 
                    if self.db_type == "mysql":
                        pass  # MySQLè¿æ¥æ± ä¼šè‡ªåŠ¨ç®¡ç†
                    else:  # sqlserver
                        self.connection_pool.release(connection)
                else:
                    # ä¸ä½¿ç”¨è¿æ¥æ± ï¼Œå…³é—­è¿æ¥
                    connection.close()
            except Exception as e:
                print(f"å…³é—­æ•°æ®åº“è¿æ¥å‡ºé”™: {e}")

    @contextmanager
    def get_cursor(self):
        """è·å–æ•°æ®åº“æ¸¸æ ‡çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        with self.get_db_connection() as connection:
            cursor = None
            try:
                cursor = connection.cursor()
                yield cursor
                connection.commit()  # æäº¤äº‹åŠ¡
            except Exception as e:
                try:
                    connection.rollback()  # å›æ»šäº‹åŠ¡
                except:
                    pass
                raise e
            finally:
                if cursor:
                    try:
                        cursor.close()
                    except:
                        pass

    def connect_to_db(self):
        """æ­¤æ–¹æ³•å·²ä¸å†ä½¿ç”¨ï¼Œä¿ç•™ä»…ä¸ºå…¼å®¹"""
        # åˆå§‹åŒ–è¿æ¥æ± 
        self.init_connection_pool()

    def train_on_database(self):
        """
        [é‡æ„] éå† `training_connections` åˆ—è¡¨ï¼Œä¸ºæ¯ä¸ªæ•°æ®åº“å»ºç«‹ç‹¬ç«‹è¿æ¥è¿›è¡Œè®­ç»ƒï¼Œ
        ä»¥ç¡®ä¿å…ƒæ•°æ®è·å–çš„å‡†ç¡®æ€§å’Œéš”ç¦»æ€§ã€‚
        """
        print("--- å¼€å§‹æ•°æ®åº“è®­ç»ƒæµç¨‹ ---")

        # [ä¿®æ”¹] å…ƒæ•°æ®å·²ç§»åŠ¨åˆ° __init__ ä¸­ä½œä¸ºå®ä¾‹å±æ€§ï¼Œæ­¤å¤„ä¸å†å®šä¹‰

        # --- [æ–°å¢] è®­ç»ƒæ‰€æœ‰å­—æ®µçš„é€šç”¨ä¸šåŠ¡çŸ¥è¯† ---
        print("\n--- æ­£åœ¨è®­ç»ƒé€šç”¨çš„å­—æ®µä¸šåŠ¡çŸ¥è¯† ---")
        try:
            field_doc_str = "ä»¥ä¸‹æ˜¯æ•°æ®åº“ä¸­ä¸€äº›å¸¸è§å­—æ®µçš„ä¸šåŠ¡å«ä¹‰è§£é‡Šï¼š\n"
            field_doc_str += "\n".join([f"- å­—æ®µ `{field}` ä»£è¡¨ '{desc}'" for field, desc in self.field_mappings.items()])
            super().train(documentation=field_doc_str)
            print("âœ… é€šç”¨å­—æ®µä¸šåŠ¡çŸ¥è¯†è®­ç»ƒå®Œæˆã€‚")
        except Exception as e:
            print(f"[é”™è¯¯] è®­ç»ƒé€šç”¨å­—æ®µä¸šåŠ¡çŸ¥è¯†æ—¶å¤±è´¥: {e}")

        # [ä¿®æ”¹] ä¸å†éå† training_connectionsï¼Œåªä½¿ç”¨ä¸»è¿æ¥è®­ç»ƒ DDL
        if not self.db_primary_connection_config:
            print("[è­¦å‘Š] `database` æˆ– `database.primary_connection` æœªé…ç½®ï¼Œè·³è¿‡ DDL è®­ç»ƒã€‚")
            print("\n--- æ•°æ®åº“è®­ç»ƒæµç¨‹å·²å®Œæˆ ---")
            return

        conn_config = self.db_primary_connection_config
        db_name = conn_config.get('database') or conn_config.get('dbname') # å…¼å®¹ä¸åŒé”®å
        db_type = self._detect_db_type(conn_config)
        print(f"\n--- æ­£åœ¨ä¸ºä¸»è¦ä¸šåŠ¡æ•°æ®åº“ '{db_name}' ({db_type}) è¿›è¡Œ DDL è®­ç»ƒ ---")

        try:
            # [æ ¸å¿ƒä¿®æ”¹] ä¸ºå½“å‰æ•°æ®åº“å»ºç«‹ä¸€ä¸ªä¸´æ—¶çš„ã€ç‹¬ç«‹çš„è¿æ¥
            with self._get_temp_connection(conn_config) as temp_conn:
                cursor = temp_conn.cursor()
                
                # 1. è·å–å½“å‰åº“çš„æ‰€æœ‰è¡¨
                all_tables_in_db = self._get_tables_for_db(cursor, db_type, db_name)
                print(f"åœ¨æ•°æ®åº“ '{db_name}' ä¸­æ‰¾åˆ° {len(all_tables_in_db)} ä¸ªè¡¨: {all_tables_in_db}")

                # 2. éå†å¹¶è®­ç»ƒæ¯ä¸ªè¡¨çš„ DDL
                for schema, table in all_tables_in_db:
                    # å‡†å¤‡ä¸šåŠ¡å…ƒæ•°æ®æ³¨é‡Š
                    metadata_key = self._get_metadata_key(table)
                    metadata = self.table_metadata.get(metadata_key)
                    ddl_header = self._build_ddl_header(metadata)

                    # è·å–å¹¶ä¸°å¯ŒDDL
                    ddl = self._get_ddl_for_table(cursor, db_type, db_name, schema, table, self.field_mappings)
                    if ddl:
                        enriched_ddl = f"{ddl_header}{ddl}"
                        print(f"è®­ç»ƒDDL: {enriched_ddl.strip()}")
                        super().train(ddl=enriched_ddl)
                        
                        # --- [æ ¸å¿ƒä¿®æ”¹] ä¸ºæ¯ä¸ªè¡¨é¢å¤–è®­ç»ƒä¸€æ¡æ¸…æ™°çš„ã€å¯æ£€ç´¢çš„ä¸šåŠ¡çŸ¥è¯† ---
                        if metadata:
                            table_doc = f"è¡¨ '{table}' (ä¸šåŠ¡å: {metadata.get('business_name', 'N/A')}) ç”¨äº: {metadata.get('description', 'N/A')}. ä¸å…¶ä»–è¡¨çš„å…³è”å…³ç³»: {metadata.get('relations', 'N/A')}"
                            print(f"è®­ç»ƒä¸šåŠ¡çŸ¥è¯†: {table_doc}")
                            super().train(documentation=table_doc)
                        # --- [æ ¸å¿ƒä¿®æ”¹ç»“æŸ] ---
                    else:
                        print(f"è­¦å‘Š: æœªèƒ½è·å–è¡¨ {db_name}.{schema}.{table} çš„DDL")
                
                cursor.close()
        except Exception as e:
            print(f"[é”™è¯¯] è®­ç»ƒæ•°æ®åº“ '{db_name}' æ—¶å¤±è´¥: {e}")
        
        print("\n--- æ•°æ®åº“è®­ç»ƒæµç¨‹å·²å®Œæˆ ---")

    # --- [æ–°å¢] ä»¥ä¸‹ä¸º train_on_database çš„è¾…åŠ©æ–¹æ³• ---

    @contextmanager
    def _get_temp_connection(self, conn_config: dict):
        """æ ¹æ®ç»™å®šçš„é…ç½®åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„æ•°æ®åº“è¿æ¥ã€‚"""
        conn = None
        db_type = self._detect_db_type(conn_config)
        try:
            if db_type == "mysql":
                conn = mysql.connector.connect(**conn_config)
            elif db_type == "sqlserver":
                conn_str_parts = []
                if 'driver' in conn_config: conn_str_parts.append(f"DRIVER={conn_config['driver']}")
                if 'server' in conn_config: conn_str_parts.append(f"SERVER={conn_config['server']}")
                if 'database' in conn_config: conn_str_parts.append(f"DATABASE={conn_config['database']}")
                if 'uid' in conn_config: conn_str_parts.append(f"UID={conn_config['uid']}")
                if 'pwd' in conn_config: conn_str_parts.append(f"PWD={conn_config['pwd']}")
                conn_str_parts.append("TrustServerCertificate=yes")
                conn_str_parts.append("Encrypt=yes")
                conn_str = ";".join(conn_str_parts)
                conn = pyodbc.connect(conn_str, timeout=5)
            
            yield conn
        finally:
            if conn:
                conn.close()

    def _get_tables_for_db(self, cursor, db_type: str, db_name: str) -> list:
        """è·å–æŒ‡å®šæ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨ã€‚è¿”å› [(schema, table_name), ...]"""
        if db_type == "mysql":
            cursor.execute("SHOW TABLES")
            return [(None, table[0]) for table in cursor.fetchall()]
        elif db_type == "sqlserver":
            # å¯¹äºSQL Serverï¼Œå› ä¸ºè¿æ¥å·²ç»æŒ‡å‘ç‰¹å®šDBï¼Œæ‰€ä»¥ä¸éœ€è¦db_nameå‰ç¼€
            query = "SELECT TABLE_SCHEMA, TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'"
            cursor.execute(query)
            return cursor.fetchall()
        return []

    def _get_metadata_key(self, table_name: str) -> str:
        """å¤„ç†åˆ†è¡¨æƒ…å†µï¼Œè¿”å›ç”¨äºå…ƒæ•°æ®å­—å…¸çš„é”®ã€‚"""
        if table_name.startswith("dat_station_hour"):
            return "dat_station_hour"
        if table_name.startswith("dat_city_hour"):
            return "dat_city_hour"
        return table_name

    def _build_ddl_header(self, metadata: dict) -> str:
        """æ„å»ºDDLçš„æ³¨é‡Šå¤´ã€‚"""
        if not metadata:
            return ""
        return f"-- ä¸šåŠ¡åç§°: {metadata.get('business_name', 'N/A')}\n-- åŠŸèƒ½æè¿°: {metadata.get('description', 'N/A')}\n-- å…³è”å…³ç³»: {metadata.get('relations', 'N/A')}\n"

    def _get_ddl_for_table(self, cursor, db_type: str, db_name: str, schema: str, table: str, field_mappings: dict) -> str:
        """ä¸ºå•ä¸ªè¡¨ç”ŸæˆDDLå­—ç¬¦ä¸²ï¼Œå¹¶é™„å¸¦å­—æ®µæ³¨é‡Šã€‚"""
        try:
            if db_type == "mysql":
                cursor.execute(f"SHOW CREATE TABLE `{table}`")
                return cursor.fetchone()[1]
            elif db_type == "sqlserver":
                # å¯¹äºSQL Serverï¼ŒDDLéœ€è¦åŒ…å«æ•°æ®åº“å’Œschemaåä»¥ç¡®ä¿å…¨å±€å”¯ä¸€
                full_table_name = f"[{db_name}].[{schema}].[{table}]"
                col_sql = f"SELECT COLUMN_NAME, DATA_TYPE, CHARACTER_MAXIMUM_LENGTH, IS_NULLABLE FROM [{db_name}].INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{table}' AND TABLE_SCHEMA = '{schema}' ORDER BY ORDINAL_POSITION"
                cursor.execute(col_sql)
                columns_info = cursor.fetchall()

                ddl_parts = []
                for col in columns_info:
                    col_name, data_type, char_max_len, is_nullable = col
                    col_def = f"  [{col_name}] {data_type}"
                    if char_max_len is not None: col_def += f"({char_max_len})"
                    if is_nullable == 'NO': col_def += " NOT NULL"
                    
                    # [æ–°å¢] æ·»åŠ å­—æ®µçš„ä¸­æ–‡æ³¨é‡Š
                    comment = field_mappings.get(col_name.lower())
                    if comment:
                        col_def += f" -- {comment}"

                    ddl_parts.append(col_def)

                pk_sql = f"SELECT KCU.COLUMN_NAME FROM [{db_name}].INFORMATION_SCHEMA.TABLE_CONSTRAINTS AS TC JOIN [{db_name}].INFORMATION_SCHEMA.KEY_COLUMN_USAGE AS KCU ON TC.CONSTRAINT_NAME = KCU.CONSTRAINT_NAME WHERE TC.TABLE_NAME = '{table}' AND TC.TABLE_SCHEMA = '{schema}' AND TC.CONSTRAINT_TYPE = 'PRIMARY KEY' ORDER BY KCU.ORDINAL_POSITION"
                cursor.execute(pk_sql)
                pk_columns = [col[0] for col in cursor.fetchall()]
                if pk_columns:
                    ddl_parts.append(f"  PRIMARY KEY ({', '.join([f'[{col}]' for col in pk_columns])})")

                ddl_content = ',\n'.join(ddl_parts)
                return f"CREATE TABLE {full_table_name} (\n{ddl_content}\n);"
        except Exception as e:
            print(f"ä¸ºè¡¨ {db_name}.{schema}.{table} ç”ŸæˆDDLæ—¶å‡ºé”™: {e}")
            return None

    def add_sql_example(self, question: str, sql: str):
        super().train(question=question, sql=sql)

    def get_sample_data(self):
        """
        ä»æ¯ä¸ªè¡¨ä¸­æŠ½å–1ä¸ªæ ·æœ¬æ•°æ®ï¼ˆæœ€å¤š10è¡Œï¼‰
        è¿”å›æ ¼å¼ï¼š{
            "table_name": [
                {"column1": value1, "column2": value2, ...},
                ...
            ],
            ...
        }
        """
        sample_data = {}
        
        try:
            with self.get_cursor() as cursor:
                # [ä¿®æ”¹] ä»æ‰€æœ‰é…ç½®çš„æ•°æ®åº“ä¸­è·å–è¡¨å
                all_tables = []
                if self.db_type == "mysql":
                    for db_name in self.db_training_connections:
                        try:
                            cursor.execute(f"USE `{db_name}`")
                            cursor.execute("SHOW TABLES")
                            # å­˜å‚¨ (æ•°æ®åº“å, schemaå, è¡¨å)
                            tables_in_db = [(db_name, None, table[0]) for table in cursor.fetchall()]
                            all_tables.extend(tables_in_db)
                        except Exception as e:
                            print(f"æ— æ³•è®¿é—®MySQLæ•°æ®åº“ '{db_name}': {e}")
                elif self.db_type == "sqlserver":
                    for db_name in self.db_training_connections:
                        try:
                            query = f"SELECT TABLE_SCHEMA, TABLE_NAME FROM [{db_name}].INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'"
                            cursor.execute(query)
                            tables_in_db = [(db_name, schema, table) for schema, table in cursor.fetchall()]
                            all_tables.extend(tables_in_db)
                        except Exception as e:
                            print(f"æ— æ³•ä»SQL Serveræ•°æ®åº“ '{db_name}' è·å–è¡¨: {e}")
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {self.db_type}")
                    
                # å¤„ç†æ¯ä¸ªè¡¨
                for db_name, schema, table in all_tables:
                    try:
                        # [ä¿®æ”¹] ä½¿ç”¨å®Œå…¨é™å®šåæ¥å”¯ä¸€æ ‡è¯†è¡¨
                        full_table_name_key = f"{db_name}.{schema}.{table}" if schema else f"{db_name}.{table}"
                        
                        # æ ¹æ®æ•°æ®åº“ç±»å‹æ„å»ºæŸ¥è¯¢
                        if self.db_type == "mysql":
                            sample_query = f"SELECT * FROM `{db_name}`.`{table}` LIMIT 10"
                        elif self.db_type == "sqlserver":
                            sample_query = f"SELECT TOP 10 * FROM [{db_name}].[{schema}].[{table}]"
                        
                        # æ‰§è¡ŒæŸ¥è¯¢
                        cursor.execute(sample_query)
                        
                        # è·å–åˆ—å
                        columns = [column[0] for column in cursor.description]
                        
                        # è·å–ç»“æœå¹¶è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                        rows = cursor.fetchall()
                        table_results = []
                        for row in rows:
                            table_results.append(dict(zip(columns, row)))
                        
                        if table_results:  # åªæœ‰å½“è¡¨æœ‰æ•°æ®æ—¶æ‰æ·»åŠ 
                            sample_data[full_table_name_key] = table_results
                    except Exception as err:
                        print(f"ä»è¡¨ {full_table_name_key} è·å–æ ·æœ¬æ•°æ®æ—¶å‡ºé”™: {err}")
                        continue
                
        except Exception as err:
            print(f"è·å–æ ·æœ¬æ•°æ®æ—¶å‡ºé”™: {err}")
            
        return sample_data

    def _perform_rag_retrieval(self, question: str, **kwargs) -> dict:
        """
        [é‡æ„] é‡‡ç”¨ç¡®å®šæ€§ç¨‹åºåŒ–åŒ¹é…ç«™ç‚¹ä¿¡æ¯ï¼Œå¹¶ç»“åˆRAGæ£€ç´¢å…¶ä»–çŸ¥è¯†çš„ç­–ç•¥ã€‚
        """
        print("--- å¼€å§‹ä½¿ç”¨æ–°çš„æ··åˆæ£€ç´¢ç­–ç•¥ï¼ˆç¨‹åºåŒ–åŒ¹é… + RAGï¼‰ ---")

        # æ­¥éª¤ 1ï¼šé—®é¢˜æ„å›¾åˆ†æä¸ç»“æ„åŒ–å®ä½“æå–
        structured_entities = self._extract_entities_for_rag(question)
        
        # æ­¥éª¤ 2: [æ ¸å¿ƒä¿®æ”¹] é€šè¿‡ç¨‹åºåŒ–æ–¹å¼è·å–ç«™ç‚¹ä¿¡æ¯ä¸Šä¸‹æ–‡
        station_info_context_str = self._get_station_info_context_programmatically(structured_entities)

        # æ­¥éª¤ 3: å¹¶è¡Œæ‰§è¡Œå‰©ä½™çš„RAGæ£€ç´¢ï¼ˆDDL, é€šç”¨æ–‡æ¡£, SQLï¼‰
        all_entities = [item for sublist in structured_entities.values() for item in sublist]
        general_entities_query = ' '.join(all_entities) if all_entities else question
        print(f"å®ä½“ç”¨äºDDLå’ŒSQLæ£€ç´¢: {general_entities_query}")

        # æ„å»ºä¸“é—¨ç”¨äºé€šç”¨æ–‡æ¡£(ä¸šåŠ¡çŸ¥è¯†)æ£€ç´¢çš„æŸ¥è¯¢
        general_query_terms = structured_entities.get('other_terms', []) + structured_entities.get('columns', [])
        doc_query = ' '.join(general_query_terms) if general_query_terms else question
        print(f"å®ä½“ç”¨äºé€šç”¨æ–‡æ¡£æ£€ç´¢: {doc_query}")

        # ä» self.kwargs (config.yaml) ä¸­è·å–å¬å›æ•°é‡ï¼Œå¹¶æä¾›é»˜è®¤å€¼
        n_ddl = self.kwargs.get('n_ddl', 4)
        n_docs = self.kwargs.get('n_docs', 10)
        n_sql = self.kwargs.get('n_sql', 4)
        print(f"RAGå¬å›æ•°é‡é…ç½®: DDL={n_ddl}, Docs={n_docs}, SQL={n_sql}")

        # å®šä¹‰å¹¶è¡Œæ£€ç´¢ä»»åŠ¡
        ddl_results = []
        doc_results = []
        sql_results = []

        def retrieve_ddl():
            all_ddls = self.get_related_ddl(question=general_entities_query, n_results=n_ddl)
            if all_ddls:
                ddl_results.extend(all_ddls)

        def retrieve_docs():
            # æ­¤å‡½æ•°ç°åœ¨åªæ£€ç´¢é€šç”¨çš„ä¸šåŠ¡çŸ¥è¯†ï¼Œå› ä¸ºç«™ç‚¹ä¿¡æ¯å·²é€šè¿‡ç¨‹åºåŒ–æ–¹å¼å¤„ç†
            all_docs = self.get_related_documentation(question=doc_query, n_results=n_docs)
            if all_docs:
                doc_results.extend(all_docs)

        def retrieve_sql():
            sql_list = self.get_similar_question_sql(question=general_entities_query, n_results=n_sql)
            if sql_list:
                # æ ¼å¼åŒ–
                formatted_sqls = [f"Q: {item['question']}\nA: {item['sql']}" for item in sql_list]
                sql_results.extend(formatted_sqls)

        threads = [
            Thread(target=retrieve_ddl),
            Thread(target=retrieve_docs),
            Thread(target=retrieve_sql),
        ]

        # å¯åŠ¨å¹¶ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for t in threads:
            t.start()
        for t in threads:
            t.join()

        # æ­¥éª¤ 4ï¼šç»„åˆå¹¶è¿‡æ»¤RAGä¸Šä¸‹æ–‡
        print(f"âœ… å·²æˆåŠŸæ£€ç´¢åˆ° {len(ddl_results)} æ¡DDLä¿¡æ¯ã€‚")
        print(f"âœ… å·²æˆåŠŸæ£€ç´¢åˆ° {len(doc_results)} æ¡é€šç”¨ä¸šåŠ¡çŸ¥è¯†ã€‚")
        print(f"âœ… å·²æˆåŠŸæ£€ç´¢åˆ° {len(sql_results)} æ¡ç›¸ä¼¼SQLæŸ¥è¯¢ã€‚")
        
        ddl_context_str = "\n---\n".join(list(dict.fromkeys(ddl_results)))
        doc_context_str = "\n---\n".join(list(dict.fromkeys(doc_results)))
        sql_context_str = "\n---\n".join(list(dict.fromkeys(sql_results)))

        rag_context_obj = {
            "station_info_context": station_info_context_str,
            "ddl_context": ddl_context_str,
            "doc_context": doc_context_str,
            "sql_context": sql_context_str,
        }

        if self.debug_mode:
            debug_context_str = self._format_rag_context_for_prompt(rag_context_obj)
            print("\n" + "="*50)
            print("ğŸ•µï¸  DEBUGGER: RAG CONTEXT TO BE SENT TO LLM ğŸ•µï¸")
            print("="*50)
            print(debug_context_str)
            print("\n" + "="*50 + "\n")
        
        return rag_context_obj

    def generate_sql(self, question: str, history: list = None, **kwargs) -> tuple[str, str, dict]:
        """
        [é‡æ„] é‡‡ç”¨å¤šè·¯å¹¶è¡Œã€å„å¸å…¶èŒçš„æ··åˆæ£€ç´¢ç­–ç•¥ç”ŸæˆSQLã€‚
        è¯¥æ–¹æ³•ç°åœ¨ä¸»è¦è´Ÿè´£ç¼–æ’ï¼Œæ ¸å¿ƒRAGé€»è¾‘å·²ç§»è‡³ _perform_rag_retrievalã€‚
        """
        # --- æ­¥éª¤ 1: åŠ è½½å¤–éƒ¨çš„æç¤ºè¯æ¨¡æ¿ ---
        try:
            with open('prompt_template.txt', 'r', encoding='utf-8') as f:
                prompt_template = f.read()
        except FileNotFoundError:
            print("[é”™è¯¯] prompt_template.txt æ–‡ä»¶æœªæ‰¾åˆ°ã€‚æ— æ³•ç»§ç»­ç”ŸæˆSQLã€‚")
            return '{"error": "Configuration file prompt_template.txt not found."}', '{"error": "Configuration file prompt_template.txt not found."}', {}

        # --- æ­¥éª¤ 2: æ‰§è¡ŒRAGæ£€ç´¢ ---
        rag_context_obj = self._perform_rag_retrieval(question, **kwargs)

        # --- æ­¥éª¤ 3ï¼šå°†åŠ¨æ€ä¸Šä¸‹æ–‡æ³¨å…¥åˆ°æ¨¡æ¿ä¸­ ---
        try:
            # [æ–°å¢] è·å–å½“å‰ä¸»è¿æ¥çš„æ•°æ®åº“å
            primary_db_name = self.db_primary_connection_config.get('database') or self.db_primary_connection_config.get('dbname', 'default_db')

            final_prompt = prompt_template.format(
                primary_database_name=primary_db_name,  # æ³¨å…¥ä¸»æ•°æ®åº“å
                db_type=self.db_type or 'æ•°æ®åº“',
                station_info_context=rag_context_obj["station_info_context"],
                ddl_context=rag_context_obj["ddl_context"],
                doc_context=rag_context_obj["doc_context"],
                sql_context=rag_context_obj["sql_context"],
                history_context=self.stringify_history(history),
                question=question  # ç¡®ä¿questionå‚æ•°è¢«æ­£ç¡®ä¼ é€’
            )
        except KeyError as e:
            print(f"[é”™è¯¯] æ ¼å¼åŒ–æç¤ºæ¨¡æ¿æ—¶å‡ºé”™: {e}. æ£€æŸ¥prompt_template.txtä¸­çš„å ä½ç¬¦æ˜¯å¦ä¸ä»£ç ä¸­çš„å‚æ•°åŒ¹é…ã€‚")
            return f'{{"error": "Error formatting prompt template: {e}"}}', f'{{"error": "Error formatting prompt template: {e}"}}', rag_context_obj
        except Exception as e:
            print(f"[é”™è¯¯] æ ¼å¼åŒ–æç¤ºæ¨¡æ¿æ—¶å‡ºç°æœªçŸ¥é”™è¯¯: {e}")
            return f'{{"error": "Unknown error formatting prompt template: {e}"}}', f'{{"error": "Unknown error formatting prompt template: {e}"}}', rag_context_obj

        # --- æ­¥éª¤ 4: å‡†å¤‡å¹¶æäº¤ç»™å¤§æ¨¡å‹ ---
        messages = self._get_common_messages()
        messages.append({"role": "user", "content": final_prompt})

        print("æ­£åœ¨æäº¤ç»™å¤§æ¨¡å‹ç”Ÿæˆæœ€ç»ˆSQL...")
        llm_response = self.submit_prompt(messages)
        
        print("æ”¶åˆ°å¤§æ¨¡å‹å“åº”ï¼Œæ­£åœ¨æ¸…ç†...")
        sql = self._clean_llm_response(llm_response)
        print(f"æ¨¡å‹å·²ç”ŸæˆSQL: {sql}")

        # [ä¿®å¤] åŒæ—¶è¿”å› rag_context_obj ä»¥ä¾¿ä¸Šå±‚å‡½æ•°åœ¨éœ€è¦æ—¶å¯ä»¥å¤ç”¨
        return sql, llm_response, rag_context_obj

    def ask(self, question: str, history: list = None, **kwargs) -> str:
        # è¿™ä¸ªæ–¹æ³•ç°åœ¨ä¼šè°ƒç”¨æˆ‘ä»¬é‡å†™è¿‡çš„ generate_sql
        # æ³¨æ„ï¼šå®ƒçš„è¿”å›å€¼å¯èƒ½æ˜¯ SQL å­—ç¬¦ä¸²ï¼Œä¹Ÿå¯èƒ½æ˜¯åŒ…å«æ¾„æ¸…é—®é¢˜çš„å­—å…¸
        return self.generate_sql(question=question, history=history, **kwargs)[0]

    def _execute_plan(self, plan_obj: dict) -> tuple[dict, any]:
        """
        [æ–°å¢] æ‰§è¡Œä¸€ä¸ªç»“æ„åŒ–çš„æŸ¥è¯¢è®¡åˆ’ã€‚
        """
        results_context = {}
        final_result = {"status": "success", "results": {}, "error": None}
        
        # å®šä¹‰ä¸€ä¸ªpysqldfå¯ç”¨çš„å±€éƒ¨å‡½æ•°ç¯å¢ƒ
        # è¿™ç§æ–¹å¼æ¯”ç›´æ¥ä½¿ç”¨ locals() æˆ– globals() æ›´å®‰å…¨
        pysqldf_env = {"pd": pd} 

        try:
            # æŒ‰ç…§è®¡åˆ’é¡ºåºæ‰§è¡Œ
            for step in sorted(plan_obj.get("plan", []), key=lambda x: x["step"]):
                step_query = step["query"]
                output_var = step["output_variable"]
                print(f"æ­£åœ¨æ‰§è¡Œè®¡åˆ’æ­¥éª¤ {step['step']}: {step['description']}")
                print(f"æŸ¥è¯¢: {step_query}")

                # æ£€æŸ¥æ˜¯å¦æ˜¯é’ˆå¯¹DataFrameçš„æŸ¥è¯¢
                if '@' in step_query:
                    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ‰€æœ‰ @variable å¼•ç”¨
                    referenced_vars = re.findall(r'@(\w+)', step_query)
                    
                    # å‡†å¤‡pysqldfçš„ç¯å¢ƒï¼Œå°†DataFrameæ³¨å…¥
                    for var in referenced_vars:
                        if var in results_context:
                            # å°†DataFrameèµ‹å€¼ç»™ä¸€ä¸ªä¸å˜é‡ååŒåçš„é”®
                            pysqldf_env[var] = results_context[var]
                        else:
                            raise ValueError(f"è®¡åˆ’æ‰§è¡Œé”™è¯¯: æ­¥éª¤ {step['step']} å¼•ç”¨äº†æœªå®šä¹‰çš„å˜é‡ @{var}")
                    
                    # ä½¿ç”¨pandasqlæ‰§è¡ŒæŸ¥è¯¢
                    # queryç°åœ¨å¯ä»¥ç›´æ¥å¼•ç”¨å˜é‡åï¼Œå› ä¸ºå®ƒä»¬åœ¨pysqldf_envä¸­
                    clean_query = step_query.replace('@', '')
                    step_df = sqldf(clean_query, pysqldf_env)
                else:
                    # æ˜¯æ™®é€šæ•°æ®åº“æŸ¥è¯¢
                    step_df = self.run_sql(step_query)
                
                # å­˜å‚¨ç»“æœ
                results_context[output_var] = step_df
                print(f"æ­¥éª¤ {step['step']} å®Œæˆï¼Œç»“æœå·²å­˜å…¥å˜é‡: {output_var}")

            # æ ¹æ® final_presentation ç»„ç»‡æœ€ç»ˆç»“æœ
            presentation = plan_obj.get("final_presentation", {})
            ordered_results = {}
            if presentation.get("type") == "multiple_results":
                for var_name in presentation.get("results_order", []):
                    if var_name in results_context:
                        df = results_context[var_name]
                        ordered_results[var_name] = df.to_dict('records') if df is not None else []
            final_result["results"] = ordered_results
            
            # model_response å¯ä»¥ç”¨æœ€åä¸€ä¸ªDataFrameï¼Œæˆ–ä¸€ä¸ªåŒ…å«æ‰€æœ‰ç»“æœçš„å­—å…¸
            model_response = {k: v for k, v in results_context.items()}

        except Exception as e:
            print(f"âŒ æ‰§è¡ŒæŸ¥è¯¢è®¡åˆ’æ—¶å‡ºé”™: {e}")
            final_result["status"] = "error"
            final_result["error"] = f"æ‰§è¡Œè®¡åˆ’å¤±è´¥: {e}"
            model_response = None
            
        return final_result, model_response

    def ask_and_run(self, question: str, history: list = None, visualize: bool = False) -> tuple[dict, any]:
        """
        [é‡æ„] é‡‡ç”¨æ–°çš„ã€åŸºäºè®¡åˆ’çš„æ‰§è¡Œæ¨¡å‹ã€‚
        - ç”ŸæˆSQLæˆ–æŸ¥è¯¢è®¡åˆ’ã€‚
        - å¦‚æœæ˜¯è®¡åˆ’ï¼Œåˆ™æ‰§è¡Œè®¡åˆ’ã€‚
        - å¦‚æœæ˜¯ç®€å•SQLï¼Œåˆ™ç›´æ¥æ‰§è¡Œã€‚
        - ä¸å†ä½¿ç”¨æ—§çš„ã€è„†å¼±çš„è‡ªæˆ‘ä¿®æ­£é€»è¾‘ã€‚
        """
        final_result = {"status": "error", "sql": "", "results": [], "error": "Internal Server Error"}
        model_response_data = None
        
        try:
            # --- æ­¥éª¤ 1: ç”ŸæˆSQLæˆ–æŸ¥è¯¢è®¡åˆ’ ---
            response_str, _, rag_context_obj = self.generate_sql(question=question, history=history)

            # -- æ ¸å¿ƒè°ƒè¯•ç‚¹ --
            print("\n" + "="*80)
            print("ğŸ•µï¸  DEBUGGER: RAW RESPONSE FROM LLM ğŸ•µï¸")
            print(f"---BEGIN---\n{response_str}\n---END---")
            print("="*80 + "\n")

            # --- æ­¥éª¤ 2: åˆ†æå“åº”ï¼Œç¡®å®šæ‰§è¡Œè·¯å¾„ (ä¼˜åŒ–é€»è¾‘) ---
            cleaned_response = response_str.strip()
            if cleaned_response.startswith('{'):
                # è·¯å¾„ A: å“åº”çœ‹èµ·æ¥æ˜¯JSON (æŸ¥è¯¢è®¡åˆ’æˆ–æ¾„æ¸…é—®é¢˜)
                try:
                    parsed_json = json.loads(cleaned_response)
                    
                    if isinstance(parsed_json, dict):
                        if "plan" in parsed_json:
                            print("âœ… æ£€æµ‹åˆ°æŸ¥è¯¢è®¡åˆ’ï¼Œå¼€å§‹æ‰§è¡Œ...")
                            final_result, model_response_data = self._execute_plan(parsed_json)
                            final_result["sql"] = json.dumps(parsed_json, indent=2, ensure_ascii=False)
                        
                        elif "clarification_needed" in parsed_json:
                            print("âš ï¸ æ¨¡å‹è¯·æ±‚æ¾„æ¸…ï¼Œæµç¨‹ä¸­æ­¢ã€‚")
                            final_result.update({
                                "status": "clarification_needed", 
                                "message": parsed_json["clarification_needed"],
                                "sql": None, 
                                "error": None,
                                "results": parsed_json
                            })
                            model_response_data = parsed_json
                        
                        else:
                            raise ValueError(f"æ¨¡å‹è¿”å›äº†æ— æ³•è¯†åˆ«çš„JSONæŒ‡ä»¤: {response_str}")
                    else:
                        raise ValueError(f"å“åº”æ˜¯JSONä½†ä¸æ˜¯é¢„æœŸçš„å¯¹è±¡æ ¼å¼: {response_str}")

                except json.JSONDecodeError:
                    # å¦‚æœé•¿å¾—åƒJSONä½†è§£æå¤±è´¥ï¼Œå½“ä½œç®€å•SQLå¤„ç†
                    print(f"âš ï¸ æ£€æµ‹åˆ°ç–‘ä¼¼JSONçš„å“åº”ä½†è§£æå¤±è´¥ï¼Œå°†ä½œä¸ºç®€å•SQLå¤„ç†: {response_str}")
                    sql = response_str # ä½¿ç”¨åŸå§‹å“åº”
                    final_result["sql"] = sql
                    df = self.run_sql(sql)
                    
                    final_result["results"] = df.to_dict('records') if df is not None else []
                    final_result["status"] = "success"
                    final_result["error"] = None
                    model_response_data = df

            else:
                # è·¯å¾„ B: è¿™æ˜¯ä¸€ä¸ªç®€å•çš„SQLè¯­å¥
                print("âœ… æ£€æµ‹åˆ°ç®€å•SQLè¯­å¥ï¼Œç›´æ¥æ‰§è¡Œ...")
                sql = response_str
                final_result["sql"] = sql
                df = self.run_sql(sql)
                
                final_result["results"] = df.to_dict('records') if df is not None else []
                final_result["status"] = "success"
                final_result["error"] = None
                model_response_data = df

        except Exception as e:
            # --- æ­¥éª¤ 3: ç»Ÿä¸€çš„é”™è¯¯æ•è· ---
            print(f"âŒ åœ¨ ask_and_run æµç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            final_result["status"] = "error"
            final_result["error"] = str(e)
            
        # å¯è§†åŒ–é€»è¾‘ (å¦‚æœéœ€è¦)
        if visualize and model_response_data is not None:
            # ç¡®ä¿æˆ‘ä»¬æœ‰DataFrameæ¥è¿›è¡Œå¯è§†åŒ–
            df_to_visualize = None
            if isinstance(model_response_data, pd.DataFrame):
                df_to_visualize = model_response_data
            # å¦‚æœæ˜¯è®¡åˆ’ç»“æœï¼Œå¯ä»¥å°è¯•å¯è§†åŒ–æœ€åä¸€ä¸ªDataFrame
            elif isinstance(model_response_data, dict):
                last_df_key = next(reversed(model_response_data), None)
                if last_df_key and isinstance(model_response_data[last_df_key], pd.DataFrame):
                    df_to_visualize = model_response_data[last_df_key]
            
            if df_to_visualize is not None and not df_to_visualize.empty:
                 # ... (å¯è§†åŒ–ä»£ç ä¿æŒä¸å˜)
                pass

        return final_result, model_response_data

    def run_sql(self, sql: str) -> pd.DataFrame:
        """
        [é‡æ„] æ‰§è¡Œç»™å®šçš„SQLæŸ¥è¯¢ã€‚
        æˆåŠŸæ—¶è¿”å›ä¸€ä¸ªPandas DataFrameã€‚
        å¤±è´¥æ—¶ï¼Œä¸å†æ•è·å¼‚å¸¸ï¼Œè€Œæ˜¯ç›´æ¥å‘ä¸ŠæŠ›å‡ºï¼Œç”±è°ƒç”¨è€…å¤„ç†ã€‚
        """
        with self.get_cursor() as cursor:
            print(f"Executing SQL in run_sql: {sql}")
            cursor.execute(sql)
            
            # è·å–åˆ—å
            columns = [column[0] for column in cursor.description]
            
            # è·å–ç»“æœå¹¶è½¬æ¢ä¸ºDataFrame
            rows = cursor.fetchall()
            # [æ–°å¢] å°†å…ƒç»„åˆ—è¡¨è½¬æ¢ä¸ºPandas DataFrame
            return pd.DataFrame([list(row) for row in rows], columns=columns)

    def get_training_data(self) -> list:
        df = super().get_training_data()
        return [] if df is None else df.to_dict('records')

    def remove_training_data(self, id: str) -> bool:
        return super().remove_training_data(id=id)

    def reset_connection_pool(self):
        """é‡ç½®è¿æ¥æ± çŠ¶æ€ï¼Œå…³é—­æ‰€æœ‰è¿æ¥å¹¶é‡æ–°åˆå§‹åŒ–"""
        try:
            print("æ­£åœ¨é‡ç½®æ•°æ®åº“è¿æ¥æ± ...")
            if hasattr(self, 'connection_pool') and self.connection_pool:
                if self.db_type == "sqlserver":
                    self.connection_pool.close_all()
                self.connection_pool = None
            
            # é‡æ–°åˆå§‹åŒ–
            self.init_connection_pool()
            return True
        except Exception as e:
            print(f"é‡ç½®è¿æ¥æ± å¤±è´¥: {e}")
            return False
            
    def get_connection_pool_status(self):
        """è·å–è¿æ¥æ± çŠ¶æ€ä¿¡æ¯"""
        status = {
            "enabled": self.use_connection_pool,
            "type": self.db_type,
            "max_connections": self.max_connections,
            "pool_initialized": self.connection_pool is not None
        }
        
        if self.connection_pool:
            if self.db_type == "sqlserver" and isinstance(self.connection_pool, SimpleConnectionPool):
                status.update({
                    "total_connections": len(self.connection_pool.connections),
                    "in_use_connections": len(self.connection_pool.in_use),
                    "available_connections": len(self.connection_pool.connections) - len(self.connection_pool.in_use)
                })
        
        return status

    def correct_sql(self, question: str, sql: str, error_message: str, rag_context_obj: dict, history: list = None) -> str:
        """
        [é‡æ„] åŸºäºSQLæ‰§è¡Œé”™è¯¯å’ŒåŸå§‹RAGä¸Šä¸‹æ–‡æ¥ä¿®æ­£SQLï¼Œä¸å†è¿›è¡Œæ–°çš„æ£€ç´¢ã€‚
        """
        print("å¯åŠ¨è½»é‡çº§SQLä¿®æ­£æµç¨‹...")

        # 1. æ ¼å¼åŒ–RAGä¸Šä¸‹æ–‡å’Œå†å²å¯¹è¯ (ä¸ generate_sql ä¿æŒä¸€è‡´)
        rag_context_str = self._format_rag_context_for_prompt(rag_context_obj)
        history_str = self.stringify_history(history)
        
        # 2. æ„å»ºä¿®æ­£Prompt
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„SQLä¿®æ­£æœºå™¨äººã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·æä¾›çš„åŸå§‹é—®é¢˜ã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€å¤±è´¥çš„SQLæŸ¥è¯¢ä»¥åŠæ•°æ®åº“è¿”å›çš„é”™è¯¯ä¿¡æ¯ï¼Œæ¥ä¿®æ­£SQLã€‚

è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š
1.  ä»”ç»†åˆ†ææä¾›çš„æ‰€æœ‰ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯æ•°æ®åº“é”™è¯¯ä¿¡æ¯ï¼Œå®ƒæ˜¯å®šä½é—®é¢˜çš„å…³é”®ã€‚
2.  **ä¸è¦**è¿›è¡Œä»»ä½•æ–°çš„ä¿¡æ¯æ£€ç´¢æˆ–å‡è®¾ã€‚åªä½¿ç”¨ä¸‹é¢æä¾›çš„ä¸Šä¸‹æ–‡ã€‚
3.  è¿”å›çš„å¿…é¡»æ˜¯ä¸”ä»…æ˜¯ä¸€ä¸ªå¯ä»¥ç›´æ¥åœ¨ {self.dialect} æ•°æ®åº“ä¸Šæ‰§è¡Œçš„SQLæŸ¥è¯¢è¯­å¥ã€‚
4.  ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€æ³¨é‡Šæˆ–markdownæ ‡è®°ï¼ˆå¦‚ ```sql ... ```ï¼‰ã€‚

è¿™æ˜¯ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå®ƒåŒ…å«äº†è¡¨ç»“æ„(DDL)ã€ä¸šåŠ¡çŸ¥è¯†å’Œç›¸ä¼¼çš„æŸ¥è¯¢æ¡ˆä¾‹:
{rag_context_str}

è¿™æ˜¯ä¹‹å‰çš„å¯¹è¯å†å²:
{history_str}
"""

        user_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ä¿®æ­£SQLæŸ¥è¯¢ï¼š

- **åŸå§‹é—®é¢˜**: {question}
- **å¤±è´¥çš„SQL**: 
```sql
{sql}
```
- **æ•°æ®åº“é”™è¯¯**: {error_message}

è¯·æä¾›ä¿®æ­£åçš„SQLæŸ¥è¯¢ã€‚
"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        # 3. è°ƒç”¨LLMè¿›è¡Œä¿®æ­£
        try:
            print("æ­£åœ¨è¯·æ±‚LLMä¿®æ­£SQL...")
            corrected_sql_response = self.submit_prompt(messages)
            corrected_sql = self._clean_llm_response(corrected_sql_response)
            print(f"LLMè¿”å›çš„ä¿®æ­£åSQL: \n{corrected_sql}")
            return corrected_sql
        except Exception as e:
            print(f"é”™è¯¯: è°ƒç”¨LLMä¿®æ­£SQLæ—¶å¤±è´¥ - {e}")
            # å¦‚æœä¿®æ­£å¤±è´¥ï¼Œå¯ä»¥è¿”å›åŸå§‹çš„é”™è¯¯SQLï¼Œæˆ–è€…æŠ›å‡ºå¼‚å¸¸
            return sql

    def close(self):
        """
        å…³é—­æ‰€æœ‰æ•°æ®åº“è¿æ¥ã€‚
        """
        try:
            print("æ­£åœ¨å…³é—­æ‰€æœ‰æ•°æ®åº“è¿æ¥...")
            if hasattr(self, 'connection_pool') and self.connection_pool:
                if self.db_type == "sqlserver":
                    self.connection_pool.close_all()
                self.connection_pool = None
            
            # é‡æ–°åˆå§‹åŒ–
            self.init_connection_pool()
            return True
        except Exception as e:
            print(f"å…³é—­æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {e}")
            return False

# [æ–°å¢] æš´éœ²ä¸€ä¸ªå•ä¸€çš„ã€é¢„é…ç½®çš„å®ä¾‹
# åœ¨app.pyä¸­ï¼Œæˆ‘ä»¬å°†æ˜¾å¼è°ƒç”¨å®ƒçš„init_connection_poolæ–¹æ³•
try:
    config = load_config()
    vanna_service = MyVanna(config)
except FileNotFoundError:
    print("\n[FATAL ERROR] å¯åŠ¨å¤±è´¥ï¼šæ‰¾ä¸åˆ° `config.yaml` æ–‡ä»¶ï¼")
    # åˆ›å»ºä¸€ä¸ªæ— æ•°æ®åº“é…ç½®çš„å®ä¾‹ä»¥å…è®¸åº”ç”¨å¯åŠ¨
    vanna_service = MyVanna()
except Exception as e:
    import traceback
    print(f"\n[FATAL ERROR] å¯åŠ¨å¤±è´¥ï¼šåˆå§‹åŒ–æœåŠ¡æ—¶é‡åˆ°æœªçŸ¥é”™è¯¯ã€‚")
    traceback.print_exc()
    # åˆ›å»ºä¸€ä¸ªæ— æ•°æ®åº“é…ç½®çš„å®ä¾‹ä»¥å…è®¸åº”ç”¨å¯åŠ¨
    vanna_service = MyVanna() 